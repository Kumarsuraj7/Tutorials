{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kumarsuraj7/Tutorials/blob/main/Topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue8D3iLdC-h1"
      },
      "source": [
        "# Topic Modeling\n",
        "\n",
        "**Libraries required for topic modeling**: Pandas, gensim and pyLDAvis\n",
        "\n",
        "URL: https://towardsdatascience.com/topic-modelling-f51e5ebfb40a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKarap4EC-h6"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq_pCovwC-h7"
      },
      "source": [
        "!pip install PyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeuz22HcHbFD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeunGnCKC-h8"
      },
      "source": [
        "!pip install -U gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoJveRdNC-h9"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8XgkyiDC-h9"
      },
      "source": [
        "# import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Dependencies for Data Pre-processing\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english') \n",
        "stoplist.extend(['last','updated'])\n",
        "stoplist = set(stoplist)\n",
        "import re\n",
        "import string\n",
        "\n",
        "#Libraries for dictionary, doc_term_matrix, LDA implementation\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from pprint import pprint\n",
        "\n",
        "#Libraries for Visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#pyldavis library helps dynamic visualization of topics\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DexMqfcJC-h-"
      },
      "source": [
        "# Read Cleaned Data with StopWords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqESfahXC-h_"
      },
      "source": [
        "# Input pre-processed text \n",
        "# read data\n",
        "DF = pd.read_csv('Cleaned_Data_With_StopWords.csv')\n",
        "DF.head()\n",
        "DF['Content_nGrams'] = DF['Processed_Content']\n",
        "Processed_Content = DF['Content_nGrams']\n",
        "DF.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwwm2pD5C-iA"
      },
      "source": [
        "# Read n_grams "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izKk8j8VC-iB"
      },
      "source": [
        "def readFile(fileName):\n",
        "    \"\"\"\n",
        "    This function will read the text files passed & return the list\n",
        "    \"\"\"\n",
        "    fileObj = open(fileName, \"r\") #opens the file in read mode\n",
        "    words = fileObj.read().splitlines() #puts the file into a list\n",
        "    fileObj.close()\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2G_5LifC-iB"
      },
      "source": [
        "def read_nGrams():\n",
        "    \"\"\"\n",
        "    This function will read bigrams & trigrams and \n",
        "    return  list of n_Grams.\n",
        "    \"\"\"\n",
        "    # read  bigrams \n",
        "    original_bigram = readFile(\"bigram.txt\")\n",
        "    # read trigrams\n",
        "    original_trigram = readFile(\"trigram.txt\")\n",
        "\n",
        "    # Combined list of bigrams & trigrams\n",
        "    n_grams_to_use = []\n",
        "    n_grams_to_use.extend(original_bigram)\n",
        "    n_grams_to_use.extend(original_trigram)\n",
        "    return n_grams_to_use\n",
        "n_grams_to_use = read_nGrams()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61wfL2pNC-iC"
      },
      "source": [
        "# Generating combined n_Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5nOUVdiC-iC"
      },
      "source": [
        "# Combine each n_Gram using '_'\n",
        "def combined_n_Grams(n_grams_to_use):\n",
        "    \"\"\"\n",
        "    This function will read n_Grams & return list of combined n_Grams using '_'\n",
        "    \"\"\"\n",
        "    Combined_nGrams = []\n",
        "    for i in range(len(n_grams_to_use)):\n",
        "        Combined_nGrams.append(n_grams_to_use[i].replace(' ','_'))\n",
        "    return Combined_nGrams\n",
        "Combined_nGrams = combined_n_Grams(n_grams_to_use) \n",
        "Combined_nGrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgFvNCBNC-iD"
      },
      "source": [
        "# Mapping of combined n_Grams to that of individual n_Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F29tvM1EC-iD"
      },
      "source": [
        "def mapping(n_grams_to_use, Combined_nGrams):\n",
        "    \"\"\"\n",
        "    This function will map combined n_Grams with that of individual n_Grams & return the dictionary.\n",
        "    \"\"\"\n",
        "    dic=dict()\n",
        "    for i in range(len(Combined_nGrams)):\n",
        "        dic[n_grams_to_use[i]] = Combined_nGrams[i]\n",
        "    return dic\n",
        "Mapping = mapping(n_grams_to_use, Combined_nGrams)\n",
        "Mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-pAErq-C-iD"
      },
      "source": [
        "## Step1: Add n-grams back into the reviews\n",
        "\n",
        "To add n-grams into the reviews. The input data has a list of n-grams generated in collocation step. They need to be replaced back into the data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RUIF722C-iE"
      },
      "source": [
        "def add_ngrams_to_input(Processed_content,Mapping):\n",
        "    \"\"\"\n",
        "    This function will replace original occurrence of n_Grams in the text with that of Combined n_Grams.\n",
        "    \"\"\"\n",
        "    for i in range(len(Processed_content)):\n",
        "        for key, value in Mapping.items():\n",
        "            Processed_content[i] = Processed_content[i].replace(key, value)\n",
        "    return Processed_content\n",
        "content_nGrams = add_ngrams_to_input(Processed_Content,Mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAs2i3q6C-iF"
      },
      "source": [
        "DF.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRKrDJQnC-iF"
      },
      "source": [
        "## Step2: Remove Stopwords from the input text\n",
        "\n",
        "There is a need to remove stopwords from the input text because such words doesn't play any role in defining topics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OjzXHHDC-iF"
      },
      "source": [
        "def removing_stopwords(text):\n",
        "    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n",
        "       & they can be remove safely without comprimising meaning of the sentence.\n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: Text after omitted all stopwords.\n",
        "        \n",
        "    Example: \n",
        "    Input : This is Kajal from delhi who came here to study.\n",
        "    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n",
        "    \n",
        "   \"\"\"\n",
        "    # repr() function actually gives the precise information about the string\n",
        "    text = repr(text)\n",
        "    # Text without stopwords\n",
        "    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist]\n",
        "    # Convert list of tokens_without_stopwords to String type.\n",
        "    words_string = ' '.join(No_StopWords) \n",
        "    return words_string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFjfGDcdC-iF"
      },
      "source": [
        "## Step3: Removing Punctuations\n",
        "\n",
        "I have considered some special characters (.,?!) as valid for our future work at the time of pre-processing the data, but are they really important from topic modeling point of view. Remember in topic modeling the idea is that Documents are comprised of Topics and Topics are made of words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5kbfUTCC-iG"
      },
      "source": [
        "def removing_special_characters(text):\n",
        "    \"\"\"Removing all the special characters except the one that is passed within \n",
        "       the regex to match, as they have imp meaning in the text provided.\n",
        "   \n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: Text with removed special characters that don't require.\n",
        "        \n",
        "    Example: \n",
        "    Input : Hello, K_a_j_a_l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
        "    Output :  Hello K_a_j_a_l This is 100 05  the payment that you will recieve Is this okay\n",
        "    \n",
        "   \"\"\"\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    \n",
        "    Formatted_Text = re.sub(r\"[^a-zA-Z0-9_']+\", ' ', text) \n",
        "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "    return Formatted_Text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDfumMJDC-iG"
      },
      "source": [
        "## Step4: Tokenization\n",
        "\n",
        "Breakdown text as list of tokens to create dictionary and document term matrix for topic model.\n",
        "The results will a list of list of input text. \n",
        "\n",
        "**Resources**: wordtokenizer from nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YVFW2DsC-iG"
      },
      "source": [
        "def tokenize_text(Updated_content):\n",
        "    \"\"\"\n",
        "    This function will tokenize the word after removing stopwords & punctuations \n",
        "    and return the list of list of articles.\n",
        "    \"\"\"\n",
        "    tokenized_text = [word for word in word_tokenize(Updated_content)]\n",
        "    return tokenized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLLnK7cgC-iG"
      },
      "source": [
        "# Writing main function to merge all the preprocessing steps.\n",
        "def text_preprocessing(text,  punctuations=True,  token = True,\n",
        "                       stop_words=True, apostrophe=False, verbs=False):\n",
        "    \"\"\"\n",
        "    This function will preprocess input text and return\n",
        "    the clean text.\n",
        "    \"\"\"\n",
        "    stoplist = stopwords.words('english') \n",
        "    stoplist = set(stoplist)\n",
        "    \n",
        "    if stop_words == True: #Remove stopwords\n",
        "        Data = removing_stopwords(text)\n",
        "    \n",
        "    if punctuations == True: #remove punctuations\n",
        "        Data = removing_special_characters(Data)\n",
        "        \n",
        "    if token == True: # Tokenize text\n",
        "        Data = tokenize_text(Data)  \n",
        "    if apostrophe == True: #Remove apostrophes\n",
        "        Data = remove_apostrophe(Data)\n",
        "    if verbs == True: #Remove Verbs\n",
        "        Data = remove_verbs(Data)\n",
        "           \n",
        "    return Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff8vOUJuC-iH"
      },
      "source": [
        "# Pre-processing for Content\n",
        "List_Content = DF['Content_nGrams'].to_list()\n",
        "Final_Article = []\n",
        "Complete_Content = []\n",
        "for article in List_Content:\n",
        "    Processed_Content = text_preprocessing(article) #Cleaned text of Content attribute after pre-processing\n",
        "    Final_Article.append(Processed_Content)\n",
        "Complete_Content.extend(Final_Article)\n",
        "DF['Updated_content'] = Complete_Content\n",
        "#print(Complete_Content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5COKPuyJC-iH"
      },
      "source": [
        "DF.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-363ZWGC-iH"
      },
      "source": [
        "# Filtering of Tokens on basis of POS_Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLgfX2L0C-iH"
      },
      "source": [
        "def Pos_tagging(text):\n",
        "    \"\"\"\n",
        "    This function will tag part of speeches corresponding to every tokens in the Corpus using NLTK.\n",
        "    \"\"\"\n",
        "    tagged_articles=[]\n",
        "    for articles in text:\n",
        "        tagged = nltk.pos_tag(articles)\n",
        "        #print(tagged[100:150])\n",
        "        tagged_articles.append(tagged)\n",
        "    #print(tagged_articles)\n",
        "    return tagged_articles\n",
        "tagged_articles = Pos_tagging(Complete_Content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpocPQpeC-iH"
      },
      "source": [
        "### List of POS tags\n",
        "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9WX1KN1C-iH"
      },
      "source": [
        "def remove_POS(text):\n",
        "    \"\"\"\n",
        "    This function will check for all POS tags and tell about necessary & unnecessary tags.\n",
        "    \"\"\"\n",
        "    cardinals=[]\n",
        "    Modal=[]\n",
        "    Adverb=[]\n",
        "    Adjective=[]\n",
        "    Preposition=[]\n",
        "    Verb=[]\n",
        "    Verbs=[]\n",
        "    Verbz=[]\n",
        "    POS=[]\n",
        "    check=[]\n",
        "    for articles in text:\n",
        "        for i, (token, POS_tag) in enumerate(articles):\n",
        "            if (POS_tag=='CD'):                \n",
        "                cardinals.append(token) # Can be dropped\n",
        "            elif (POS_tag=='MD'):\n",
        "                Modal.append(token) # Can be dropped to improve results\n",
        "            elif (POS_tag=='RB'):\n",
        "                Adverb.append(token) # Can't drop them\n",
        "            elif (POS_tag=='JJ'):\n",
        "                Adjective.append(token) # Can't drop them\n",
        "            elif (POS_tag=='IN'):\n",
        "                Preposition.append(token) # Can't drop them\n",
        "            elif (POS_tag=='VB'):\n",
        "                Verb.append(token) # Can't drop them but we can provide some of the tokens in stoplist to remove.\n",
        "            elif (POS_tag=='VBG'):\n",
        "                Verbs.append(token) #These verbs can be dropped (unique-780)(total-6417) (need to see again)\n",
        "            elif (POS_tag=='VBZ'):\n",
        "                Verbz.append(token) #Can't be dropped\n",
        "            elif (POS_tag=='POS'):\n",
        "                POS.append(token) # Should drop it.\n",
        "            elif (POS_tag=='PRP'):\n",
        "                check.append(token) # Nouns can't be dropped, coordinating junction should be dropped, take a look at DT(keep imp ones & drop rest),\n",
        "                #'FW'--> Imp to keep, Adjectives can't be dropped, 'PRP' should be dropped after keeping imp tokens.\n",
        "            \n",
        "            \n",
        "    #print(set(cardinals))\n",
        "    print(set(Modal))\n",
        "    #print(set(Adverb))\n",
        "    #print(set(Adjective))\n",
        "    #print(set(Preposition))\n",
        "    #print(set(check))\n",
        "remove_POS(tagged_articles)\n",
        "# After analysing each POS tag that tokens in my text can have, I figure out there are many tokens which are tagged by different POS_tags.\n",
        "# Nouns mostly included every tokens.\n",
        "# Shoud Remove CD,MD,POS,CJ,PRP,DT\n",
        "# Look at imp tokens and provide rest of unnecessary tokens in a stoplist --> VB,VBG,DT,PRP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GKcrA-rC-iI"
      },
      "source": [
        "def keeping_nouns_only(text): \n",
        "    \"\"\"\n",
        "    This Function will keep tokens tagged with Nouns and remove everything else from the corpus\n",
        "    & return with list of list of articles with filtered tokens.\n",
        "    \"\"\"\n",
        "    Result=[]\n",
        "    for i in range(len(text)):\n",
        "        Articles_Nouns=[]\n",
        "        for j in range(len(text[i])):\n",
        "            if (text[i][j][1] == 'NN' or text[i][j][1]=='NNP' or text[i][j][1]=='NNS' or text[i][j][1] == 'NNPS'): \n",
        "                Articles_Nouns.append(text[i][j][0])\n",
        "        Result.append(Articles_Nouns)    \n",
        "    return Result\n",
        "Result_Nouns = keeping_nouns_only(tagged_articles)\n",
        "#print(Result)\n",
        "# Looked at results after keeping nouns only and drop everything else. (Results didn't improve much.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYmozQy5C-iI"
      },
      "source": [
        "# Total no. of tokens in the corpus\n",
        "tokens = []\n",
        "for article in DF['Updated_content']:\n",
        "    for word in article:\n",
        "        tokens.append(word)\n",
        "len(tokens) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbd4qFD9C-iI"
      },
      "source": [
        "# Total no. of tokens in the NOUNS only corpus\n",
        "list_tok = []\n",
        "for article in Result_Nouns:\n",
        "    for word in article:\n",
        "        list_tok.append(word)\n",
        "len(list_tok) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJkPC1Q-C-iJ"
      },
      "source": [
        "print(\"Only Nouns Text\",len(list_tok)) \n",
        "print(\"All Text\", len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4NBH5i6C-iJ"
      },
      "source": [
        "DF['Updated_content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQqDdDljC-iJ"
      },
      "source": [
        "# Unique tokens in the corpus\n",
        "len(set(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLt2Ou3kC-iJ"
      },
      "source": [
        "## Step5: Create Dictionary and Document term matrix\n",
        "\n",
        "Use the tokenized Input of data and prepare the Dictionary and Document Term Matrix. \n",
        "\n",
        "**Resources**: gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQU-EcLtC-iJ"
      },
      "source": [
        "# define the function to create dictionary and document to term matrix\n",
        "def create_dic_and_docterm_matrix(Complete_Content, dict_file_path, matrix_file_path):\n",
        "    \"\"\"\n",
        "    This function will create corpus dictionary and document to term matrix\n",
        "    \n",
        "    Argument:\n",
        "        X: tokenized text corpus\n",
        "        dict_file_path: file path to save dictionary\n",
        "        matrix_file_path: file path to save matrix\n",
        "    returns:\n",
        "        corpus dictionary and document to term matrix\n",
        "    \"\"\"   \n",
        "    \n",
        "    # Create Dictionary\n",
        "    id2word_dic = corpora.Dictionary(Complete_Content)\n",
        "    # Save Dictionary\n",
        "    id2word_dic.save(dict_file_path)\n",
        " \n",
        "    # Create Corpus\n",
        "    text = Complete_Content # Query here(Should I keep the same corpus after tokenization or update with the one got after POS_tagging)\n",
        "    #  Document to term Frequency\n",
        "    doc_term_matrix = [id2word_dic.doc2bow(tokens) for tokens in text]\n",
        "    # Save Doc-Term matrix\n",
        "    corpora.MmCorpus.serialize(matrix_file_path, doc_term_matrix)\n",
        "\n",
        "    return id2word_dic, doc_term_matrix\n",
        "    \n",
        "    \n",
        "dict_file_path = r\"C:\\Users\\Kajal\\Desktop\\Topic Modelling\\dictionary.txt\"\n",
        "matrix_file_path = r\"C:\\Users\\Kajal\\Desktop\\Topic Modelling\\doc_term_matrix.txt\"\n",
        "dic_LDA, doc_term_matrix  = create_dic_and_docterm_matrix(Complete_Content, dict_file_path, matrix_file_path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8kpelZC-iJ"
      },
      "source": [
        "# function to load dictionary and doc to term matrix from the file\n",
        "def load_dict_and_docterm_matirx(dict_path, matrix_path):\n",
        "    \"\"\"\n",
        "    This fucntion will load and return\n",
        "    dictionary and doc term matrix\n",
        "    \n",
        "    Arguments:\n",
        "        dict_path: path to corpus dictionary\n",
        "        matrix_path: path to corpus document to term matrix\n",
        "                    \n",
        "    returns:\n",
        "    dictionary and doc-term matrix\n",
        "    \"\"\"\n",
        "\n",
        "    dictionary = corpora.Dictionary.load(dict_path)\n",
        "    doc_term_matrix = corpora.MmCorpus(matrix_path)    \n",
        "    return dictionary, doc_term_matrix\n",
        "\n",
        "dictionary, doc_term_matrix = load_dict_and_docterm_matirx(dict_file_path, matrix_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iZo7pKDC-iK"
      },
      "source": [
        "## Step6: Prepare Topic model and generate Coherence scores\n",
        "\n",
        "**Tips:** The model needs good memory and cores to train faster. Therefore select Chunksize paramter wisely. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZHjpgsC-iK"
      },
      "source": [
        "## Prepared Topic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgSXM78_C-iK"
      },
      "source": [
        "for k in range(2,25): # Train LDA on different values of k\n",
        "    print('Round: '+str(k))\n",
        "    LDA = gensim.models.ldamulticore.LdaMulticore\n",
        "    ldamodel = LDA(doc_term_matrix, num_topics=k, id2word = dictionary, passes=20, iterations=100,\n",
        "                   chunksize = 10000, eval_every = 10, random_state=20)\n",
        "    ldamodel.save(f\"ldamodel_for_{k}topics_Run_10\")\n",
        "    pprint(ldamodel.print_topics())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6yXqsddC-iK"
      },
      "source": [
        "## Generate Coherence Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg-RJlXoC-iK"
      },
      "source": [
        "coherence = []\n",
        "for k in range(2,25):\n",
        "    LDA = gensim.models.ldamulticore.LdaMulticore\n",
        "    ldamodel = LDA.load(f\"ldamodel_for_{k}topics_Run_10\")\n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts=Complete_Content, dictionary=dictionary, coherence='c_v')\n",
        "    coherence.append((k, 'default', 'default', cm.get_coherence()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Opd4HDMC-iK"
      },
      "source": [
        "pd.DataFrame(coherence, columns=['LDA_Model','alpha','eta','coherence_score']).to_csv('coherence_matrix_10.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-CgZU2C-iK"
      },
      "source": [
        "mat = pd.read_csv('coherence_matrix_10.csv')\n",
        "mat.reset_index(drop=True)\n",
        "mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm-AfncyC-iK"
      },
      "source": [
        "# Visualize Coherence score for top 25 LDA models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEaS9V_aC-iL"
      },
      "source": [
        "# Show graph\n",
        "x = range(2,25)\n",
        "plt.plot(x, mat['coherence_score'])\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show() # Num Topics = 4 is having highest coherence score."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaulnH__C-iL"
      },
      "source": [
        "LDA = gensim.models.ldamulticore.LdaMulticore\n",
        "ldamodel = LDA.load(f\"ldamodel_for_16topics_Run_10\")\n",
        "pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Ag0Z4hC-iL"
      },
      "source": [
        " # Finding the dominant topic in each Article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7G6d6ObC-iL"
      },
      "source": [
        "def finding_dominant_topic(ldamodel, corpus, tokenized_content, content_nGrams, Cleaned_text):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    sent_topics_df = pd.concat([sent_topics_df, tokenized_content, content_nGrams, Cleaned_text], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "df_topic_sents_keywords = finding_dominant_topic(ldamodel=ldamodel, corpus=doc_term_matrix, tokenized_content=DF['Updated_content'], content_nGrams = DF['Content_nGrams'], Cleaned_text=DF['Processed_Content'] )\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Article_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Processed_tokenized_Text','Text_nGrams' ,'Original_Cleaned_Text']\n",
        "\n",
        "# Show\n",
        "df_dominant_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6uqHEgRC-iL"
      },
      "source": [
        "# Each Topic distribution across  Articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhXKkK8CC-iM"
      },
      "source": [
        "#STORING ALL THE DATAFRAMES AS VALUES IN A DICTIONARY WHOSE KEYS ARE THE CORRESPONDING TOPICS\n",
        "dictionary_of_DataFrames={}\n",
        "\n",
        "grp=df_dominant_topic.groupby('Dominant_Topic')\n",
        "\n",
        "#A GROUP OBJECT WILL HAVE TWO COMPONENTS. ONE:THE VALUE OF THE ATTRIBUTE ON WHICH THE DATASET IS GROUPED, TWO: THE CRRESPONDING GROUPS FOR EACH UNIQUE VALUE OF THAT ATTRIBUTE.\n",
        "for topics, dataframes in grp:     \n",
        "    dictionary_of_DataFrames[topics]=pd.DataFrame(dataframes[['Article_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Processed_tokenized_Text','Text_nGrams' ,'Original_Cleaned_Text']] ,columns=['Article_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Processed_tokenized_Text','Text_nGrams' ,'Original_Cleaned_Text']).reset_index(drop=True)\n",
        "    \n",
        "dictionary_of_DataFrames[8.0] #Details of Articles corresponding to Topic 7."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py27rrgQC-iM"
      },
      "source": [
        "# Create csv files for each Topic representing all the corresponding articles.\n",
        "pd.DataFrame(dictionary_of_DataFrames[15.0], columns=['Article_No', 'Topic_Perc_Contrib', 'Keywords', 'Processed_Text', 'Original_Text']).to_csv('Topic_16.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yHYx9_rC-iM"
      },
      "source": [
        "## Summary of  understanding of topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNQC8NC4C-iM"
      },
      "source": [
        "- Results with default values of alpha & eta, random state=20 (good)\n",
        "- Results with alpha=0.1 and eta = 0.01 and random state = 123. (Bad)\n",
        "- Results after cleaning verbs & apostrophe marks, Keeping Nouns only (Not Improved)\n",
        "- Results with default values of alpha & eta, random state=20, chunksize=10000 (Better Results to keep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD1MfJMkC-iM"
      },
      "source": [
        "- Topic modelling refers to the task of identifying topics that best describes a set of documents. And the goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.\n",
        "\n",
        "- LDA can hardly run on big data due to memory/time issues. It will run better if you have access to machine with 64x architecture and big RAM capacity, 16 GB or more.\n",
        "\n",
        "- Values of lambda that are very close to 0 will show terms that are more specific for a chosen topic. Meaning that we will see terms that are \"important\" for that specific topic but not necessarily \"important\" for the whole corpus.\n",
        "\n",
        "- Values of lambda that are very close to 1 will show those terms that have the highest ratio between frequency of the terms for that specific topic and the overall frequency of the terms from the corpus."
      ]
    }
  ]
}